# ğŸ§  Machine Learning Zoomcamp â€“ Conceptual Notes & Assignments

This repository contains my personal notes, summaries, and assignments from the Machine Learning Zoomcamp course by DataTalksClub.

Iâ€™m documenting my learning journey with a focus on:

ğŸ“˜ Conceptual clarity for beginners  
ğŸ› ï¸ Practical assignments  
ğŸ§© Clean Markdown formatting for recruiter visibility  

---

## ğŸ“Œ Learning Progress â€“ Machine Learning Zoomcamp

---

### âœ”ï¸ Intro Section Completed

**Key learnings**:
- What Machine Learning is (and isnâ€™t)
- Differences between ML, RL, rule-based systems, and traditional programming
- The CRISP-DM framework for ML projects
- Model selection and data splitting (train/validation/test)
- Setting up the ML environment

---

### âœ”ï¸ Module 2 â€“ Regression Completed

**Key learnings**:
- Preparing and cleaning data with Pandas & NumPy
- Exploratory Data Analysis (EDA) with Seaborn/Matplotlib
- Building proper train/validation/test splits for fair evaluation
- Linear regression theory and vectorized implementation
- Feature engineering (numerical + categorical)
- Evaluating models with RMSE
- Regularization and tuning for better performance
- Using the trained model for predictions

---

### âœ”ï¸ Module 3 â€“ Classification Completed

**Key learnings**:
- Binary classification using logistic regression
- Encoding categorical features and scaling numerical ones
- Using `.predict_proba()` vs `.predict()` for decision-making
- Applying decision thresholds to convert probabilities into binary predictions
- Understanding class imbalance and its impact on accuracy
- Building reusable training and prediction pipelines
- Saving and loading models for deployment
- Preparing for evaluation with proper data splits and preprocessing discipline

---

### âœ”ï¸ Module 4 â€“ Evaluation Completed

**Key learnings**:
- Why accuracy alone is misleading in imbalanced datasets
- Using `DummyClassifier` to establish a baseline
- Understanding the confusion matrix: TP, FP, TN, FN
- Computing precision, recall, and F1 score from confusion matrix
- Tuning decision thresholds to balance precision and recall
- Plotting precision-recall curves to visualize tradeoffs
- Plotting ROC curves and computing AUC to measure ranking quality
- Evaluating individual features using ROC AUC
- Applying 5-fold cross-validation to assess model stability
- Interpreting mean and standard deviation of AUC across folds
- Documenting evaluation results with metrics, plots, and threshold decisions

---

### â¬œ Module 5 â€“ Coming Soon

Stay tuned for:
- Precision/recall tradeoffs in real-world deployment
- Threshold tuning for business alignment
- Model calibration and probability reliability
- Advanced evaluation strategies for production readiness

---

