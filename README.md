# 🧠 Machine Learning Zoomcamp – Conceptual Notes & Assignments

This repository contains my personal notes, summaries, and assignments from the Machine Learning Zoomcamp course by DataTalksClub.

I’m documenting my learning journey with a focus on:

📘 Conceptual clarity for beginners  
🛠️ Practical assignments  
🧩 Clean Markdown formatting for recruiter visibility  

---

## 📌 Learning Progress – Machine Learning Zoomcamp

---

### ✔️ Intro Section Completed

**Key learnings**:
- What Machine Learning is (and isn’t)
- Differences between ML, RL, rule-based systems, and traditional programming
- The CRISP-DM framework for ML projects
- Model selection and data splitting (train/validation/test)
- Setting up the ML environment

---

### ✔️ Module 2 – Regression Completed

**Key learnings**:
- Preparing and cleaning data with Pandas & NumPy
- Exploratory Data Analysis (EDA) with Seaborn/Matplotlib
- Building proper train/validation/test splits for fair evaluation
- Linear regression theory and vectorized implementation
- Feature engineering (numerical + categorical)
- Evaluating models with RMSE
- Regularization and tuning for better performance
- Using the trained model for predictions

---

### ✔️ Module 3 – Classification Completed

**Key learnings**:
- Binary classification using logistic regression
- Encoding categorical features and scaling numerical ones
- Using `.predict_proba()` vs `.predict()` for decision-making
- Applying decision thresholds to convert probabilities into binary predictions
- Understanding class imbalance and its impact on accuracy
- Building reusable training and prediction pipelines
- Saving and loading models for deployment
- Preparing for evaluation with proper data splits and preprocessing discipline

---

### ✔️ Module 4 – Evaluation Completed

**Key learnings**:
- Why accuracy alone is misleading in imbalanced datasets
- Using `DummyClassifier` to establish a baseline
- Understanding the confusion matrix: TP, FP, TN, FN
- Computing precision, recall, and F1 score from confusion matrix
- Tuning decision thresholds to balance precision and recall
- Plotting precision-recall curves to visualize tradeoffs
- Plotting ROC curves and computing AUC to measure ranking quality
- Evaluating individual features using ROC AUC
- Applying 5-fold cross-validation to assess model stability
- Interpreting mean and standard deviation of AUC across folds
- Documenting evaluation results with metrics, plots, and threshold decisions

---

### ⬜ Module 5 – Coming Soon

Stay tuned for:
- Precision/recall tradeoffs in real-world deployment
- Threshold tuning for business alignment
- Model calibration and probability reliability
- Advanced evaluation strategies for production readiness

---

